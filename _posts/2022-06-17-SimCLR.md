---
title: "SimCLR: A Simple Framework for Contrastive Learning of Visual Representations"
date: 2022-06-17 23:39
category: "논문-리뷰"
tag: [Self-Supervised Learning, SSL, Vision]

published: true
toc: true
toc_sticky: false
---

> [SimCLR: A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/pdf/2002.05709.pdf)

연관 포스트: [Audio Self-supervised Learning: A Survey (2) Audio]({% post_url 2022-05-30-Audio-SSL-A-Survey-(2) %})  
[CLAR]({% post_url 2022-06-15-CLAR %})  

> **Abstract**  
contrastive self-supervised learning without specialized architectures or a memory bank  
    1. predictive task에 data augmentation이 중요함  
    2. representation과 contrastive loss를 계산 중간에 learnable nonlinear transformation을 넣으면 representation이 더 좋아짐  
    3. contrastive learning은 supervised learning보다 큰 batch size, 더 긴 training step에 더 이득  

# 1. Introduction
- supervision 없이 학습하는데 주류는 2 가지: Generative or Discriminative
- **Generative**
    - 만들어내거나 input space에 pixel을 모델링
    - pixel-level generation은 expensive하고 representation learning에 필수적인 건 아닐 수 있음
- **Discriminative**
    - objective function으로 representative 학습
