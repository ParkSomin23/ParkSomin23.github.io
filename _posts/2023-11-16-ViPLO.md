---
title: "ViPLO: Vision Transformer based Pose-Conditioned Self-Loop Graph for Human-Object Interaction Detection"
date: 2023-11-16 15:00
category: "논문-리뷰"
tag: [Vison, HOI]
published: true

toc: true
toc_sticky: true
use_math: true
---

> 논문: [ViPLO: Vision Transformer based Pose-Conditioned Self-Loop Graph for Human-Object Interaction Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Park_ViPLO_Vision_Transformer_Based_Pose-Conditioned_Self-Loop_Graph_for_Human-Object_Interaction_CVPR_2023_paper.pdf) <br>
> [페이지](https://openaccess.thecvf.com/content/CVPR2023/html/Park_ViPLO_Vision_Transformer_Based_Pose-Conditioned_Self-Loop_Graph_for_Human-Object_Interaction_CVPR_2023_paper.html), [코드](https://github.com/Jeeseung-Park/ViPLO)

# Abstract
- Human-Object Interaction (HOI) detection: 사람-사물 사이의 관계
- two-stage HOI detection 
    - 장점: efficient trainig & inference
    - 단점: 오래된 backbone network에 따른 낮은 성능, interaction 분류 시에 사람의 HOI 인식 과정 고려 부족 
    - Vision Transformer (ViT) based Pose-Conditioned Self-Loop Graph (ViPLO)으로 two-stage HOI detection 장점 개승 및 단점 개선
- **masking with overlapped area (MOA) module**: backbone Vision Transformer (ViT)를 위한 새로운 feature 추출 방법
    - Vision Transformer (ViT)를 backbone으로 사용했을 때 발생하는 quantization 문제 해결을 위해 각 patch와 주어진 영역(region)이 겹친 부분에 대한 attention function을 사용 제안
- **pose-conditioned self-loop structure**를 가진 graph
    - 사람 관절에 대한 local features를 사용하여 human node encoding 업데이트
    - 분류기가 어떤 상호작용인지 판별할 때, 사람의 특정 관절에 집중하여 판단할 수 있도록 함
    - 사람의 HOI 인식 과정에서 영감 받음
<br><br>

# 개인적인 정리
## Feature Extraction
- Faster R-CNN을 활용하여 사람과 사물 찾고, 이는 ViT를 사용하여 feature 추출 (CLS token 사용)
- ResNet에서 사용하는 ROIAlign 방식을 바로 사용 불가능하기에 $\rightarrow$ MOA module 사용
    - MOA module: 주어진 영역과 각 patch가 겹치는 면적을 patch 크기에 대해 normalize하고, 이를 attention mask에 사용 + query와 key로 생성한 attention map에 log를 취해서 더해줌<br>
        quantization 문제 해결
- pose는 Vitpose network 사용하여 추출

## Pose-conditioned Graph Network
- 초기화
    - human node encoding과 object node encoding은 각각 CLS token을 mapping하여 초기화
    - edge encoding들은 사람 포즈 + 공간 정보를 mapping하여 초기화 
- pose-aware edge encoding
    - object node는 human node에 의해 update
    - human node는 human local feature와 object node에 의해 update
        - human local feature란?
            - pairwise spatial features(query)와 pairwise joint features(key)로 사람의 **각 관절에 대한 joint attention weight** 구함 
            - 각 관절에 대한 local information을 ROIAlign 방식으로 구하여 **joint local feature** 구함 (patch token 사용) 
            - joint attention weight과 joint local feature를 weighted sum하여 **각 관절이 사물에 대한 human local feature** 구함
        - human local feature는 human node와 비슷한 정보로 구성되어 있기에 self-loop 구조와 닮았다고 할 수 있음

## 한 줄 요약
MOA module을 사용하여 quantization 문제 해결하고, HOI 문제를 물건과 사람의 특정한 관절 사이의 관계성을 사용하여 사람과 물건 사이의 상호 작용 파악
<br><br>

# 1. Introduction

- HOI detection: 이미지에서 사람과 사물의 위치를 찾고, 이 둘 사이의 상호 작용을 판별이 목적
    - $<human, object, interaction>$ triplet 형태로 나타냄
    - 주로 one-stage와 two-stage 방법으로 나뉨
- one-stage method
    - PPDM이 최초로 제안: interaction points와 union boxes를 사용하여 HOI triplets 바로 예측
    - 최근에는 HOI triplets 예측을 위한 2개의 sub-task decoder로 구성된 two-branch transformer + matching process
    - 단점: 학습이 느리고, 메모리 사용량이 많음
- two-stage method
    - 방법 과정
        1. 기존에 있는(off-the-shelf) detector를 사용하여 사람과 사물 찾음
        2. 사람과 사물에 대한 특징값을 ROI-Pooling을 사용하여 추출
        3. 추출된 특징값을 사용하여 사람과 사물 사이의 상호작용 예측
    - 장점: pre-trained object detector를 사용하여 빠른 학습, 이미지에서 원하는 object bounding box를 알고 있으면 추가적인 object detection 없이 사용 가능, 원하는 사람-사물 쌍에 대한 interaction만 inference 가능
    - 단점: 성능이 안 좋음
- 제안된 논문에서는 two-stage 방식에 대한 성능을 향상시키고, 연산량과 공간 복잡도 감소까지 수행
    - **feature extraction**과 **interaction prediction**에 대한 성능 향상이 주됨

## MOA for Feature Extraction
- two-stage method들은 backbone으로 주로 ResNet 사용
- computer vision에서 SOTA인 ViT를 backbone으로 사용하여 성능 향상 기대<br>
$\rightarrow$ ResNet과 ViT의 output feature maps 크기가 다르기 때문에 ROIAlign과 같은 기존 방식을 바로 사용 불가능<br>
$\rightarrow$ masking with overlapped area (MOA) module 제안 
- attention function에서 각 patch와 주어진 영역(region)이 겹쳐진 부분을 고려하여 공간적인(spatial) quantization 문제 해결
- 계산 성능 향상: ResNet을 backbone으로 사용하는 기존 논문들과 inference 속도 비슷

## Pose-Conditioned Self-Loop Structure for Interaction Prediction
- 사람의 HOI 인식에서 object identity(객체 정체성(?)), relative positions(상대적 위치), reach motions(손을 뻗는 행동), manipulation motions(조작하는 행동), context(맥락)이 고려됨
- 사람의 HOI 인식 과정
    <p align='center'>
    <img src="../assets/images/ViPLO/img_01.jpeg" width="80%">
    </p> 

    1. 사람과 사물의 위치 찾기
    2. 사람과 사물의 공간적인 관계(spatial relationship)와 사람의 포즈를 사용하여 상호작용 식별
    3. 사람의 특정한 관절을 집중하여 어떤 종류의 상호작용인지 판단
- 사람의 HOI 인식 과정을 기반으로한 **pose-conditioned graph neutral network**을 사용하여 상호작용 예측
    - 가존 연구: 보조적인(auxiliary) network를 사용하여 상호작용이 없는(non-interactive) 쌍 제거
    - 제안된 방법: <span style='background-color:#fff5b1'>"공간적인 사람-사물 관계(spatial human-object relationship)와 사람 포즈 정보를 통해 얻은" edge encoding으로 상호작용성 표현</span><br>
    $\rightarrow$ 이를 통해 상호작용을 인식하는 (interactiveness-aware) edge encoding으로 의미있는 message passing 가능
- 특정한 관절 주목
    - 기존 연구: 국부적인(local) 포즈 정보가 담김 간단한 message passing이나 attention mechanism
    - 제안된 방법: <span style='background-color:#fff5b1'>query (공간적 특값징) / key (관절 특징값) 구조로 이루어진 pose-aware attention mechanism 사용</span><br>
    $\rightarrow$ self-loop 구조로 인하여 더 풍부한 국부적인(local) 정보를 가진 human node encodings 생성됨
<br><br>

# 2. Related Work
## One-stage Methods
- One-stage 방법은 HOI triplet을 사람과 사물을 미리 정의된 anchor를 사용하여 연관시킨 후, 그들의 상호작용을 예측
    - 기존 연구의 미리 정의된(predefined) anchor: interaction keypoints, union regions
- 최근에는 Transformer-based HOI detectors 사용<br> 
$\rightarrow$[bipartite matching과 Hungarian loss](https://gazelle-and-cs.tistory.com/29)로 학습하여 HOI detection task를 set prediction 문제로 바꿈
- 두 개의 decoder를 사용하여 각 sub-task를 해결하는 방법들도 제안되어 옴
- 단점: 느린 학습 시간, 메모리 사용량이 많음, 이미 detection 위치를 알고 있거나 특정한 사람-사물 쌍에 대한 상호작용만 파악하고 싶을 때의 불필요한 detection이 항상 수행되어야 하는 문제

## Two-stage Methods
- Two-stage 방법은 기존에 있는(off-the-shelf) detector를 사용하여 사람과 사물 찾고, 각 사람-사물 쌍에 대한 상호작용 분류
    - HO-RCNN 이후, 공간적 특징, 포즈 특징, 언어적인 특징과 같은 추가적인을 사용하여 맥락적 특징을 사용
    - graph 구조의 message passing 방법을 활용하여 전체적인(global) 맥락 정보를 활용하는 연구들도 제안됨
- 기존 연구들이 ResNet과 ROIAlign을 사용하기에 feature extraction 단계에서 개선점이 남아 있음
- DEFR에서 ViT를 backbone network로 사용하였으나, HOI detection이 아닌 HOI recognition에 집중함<br>
$\rightarrow$ <span style='background-color:#fff5b1'>quantization 문제를 다루지 않아 주어진 영역과 추출된 특징의 위치가 일치하지 않는(misalignment) 상황 발생</span>
- <span style='background-color:#fff5b1'>이전 상호작용 분류기들은 상호작용 종류에 따른 사람의 특정한 관정에 대한 초점을 두는 사람의 HOI 인식 과정을 고려하지 않았기에 이를 고려하는 graph neural network with pose-conditioned self-loop 구조 제안</span>
<br><br>

# 3. Method
<p align='center'>
<img src="../assets/images/ViPLO/img_02.jpeg" width="80%">
</p> 

## 3.1 Feature Extraction with ViT
- 먼저 (기존에 있던) Faster R-CNN을 활용하여 사람과 사물 찾고, ViT로 features 추출
    - 기존 ResNet은 ROI-Pooling이나 ROIAlign을 활용하여 feature map에 있는 visual feature 추출했으나, <br>
    ViT을 사용하면 ResNet과 output feature map의 크기가 다르기에 새로운 방법 필요 $\rightarrow$ **Masking with Overlapped Area (MOA) module**

### Masking with Overlapped Area 
<p align='center'>
<img src="../assets/images/ViPLO/img_03.jpeg" width="60%">
</p> 

- 문제
    - ViT는 CLS token과 같은 학습 가능한 embedding을 patch embedding 맨앞에 붙임 $\rightarrow$ transformer encoder $\rightarrow$ CLS token은 image representation으로 사용됨<br>
    $\Rightarrow$ 주어진 영역에서 해당하는 특징값을 추출하기 위해 CLS token 사용이 필수 
    - DEFR 방식에에서 사용된 것처럼, CLS token을 위해 주어진 영역 밖의 patch embedding 지움(mask out), 사물의 bounding box 안의 patch embedding만 참조 가능
    - 현존하는 ViT를 사용할 때 quantization 문제 (주어진 영역과 추출된 특징값의 misalignment)가 발생하는데, 이는 input patch가 주로 $14\times14, \ 16\times16,\ 32\times32$ $\rightarrow$ bounding box 좌표는 pixel 단위로 제공되고 patch 경계와 bounding box 경계가 일치하지 않은 경우 많기 떄문
- 제안된 해결 방안
    - 제안된 방법에서는 위 문제를 attention function에 있는 attention mask에 각 patch와 주어진 영역(region)이 겹쳐진 부분을 사용하여 quantization 문제 해결
    - 과정
        1. normalized overlapped area 구하기: 각 patch와 주어진 영역이 겹친 부분의 면적을 계산하고, patch의 크기에 대해 normalize
        2. normalized overlapped area가 attention mask matrix의 첫 행으로 지정
            - 이 attention mask matrix는 log를 취한 다음, self-attention layer에서 key와 query로 만들어진 attention map에 더해짐
            
            $$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}} + log(S))V$$
            
            - Q, K, V: query, key, value
            - $d_k$: Q, K, V의 dimension
            - $S$: normalized overlapped area

### Efficient computation for MOA
- 겹쳐진 영역인 $S$에 대한 연산은 모든 bounding box에 대해 연산되어야 함
- CPU가 아닌 GPU로 연산하여 속도를 높임
<p align='center'>
<img src="../assets/images/ViPLO/img_04.jpeg" width="47%">
<img src="../assets/images/ViPLO/img_04_1.jpeg" width="40%">
</p> 

- **위 이미지와 아래의 설명에서의 값은 이해를 위한 값을 추측한 것으로 실제 방식과는 다른 점이 있을 수 있습니다**
    - box coordinate <b>b</b>: [0.2, 0.5, 1.4, 2.4]<br>
    patch size $p$: 1<br>
    attention map length $L$: 9
    - $a,b,c,d = \text{b}_{int}$: [ 0, 0, 2, 3]
    - $x, y, z, w = \text{b}_{wh}$: [0.8, 0.5, 0.4, 0.4]
    - row: arange(1, 3)= [1, 2]
    - mask index: [1, 2, 1, 2, 1, 2] + 3 * [0, 0, 1, 1, 2, 2] = [1, 2, 4, 5, 7, 8]
    - area_row A_r: [0.8, 0.4]
    - area_column A_c: [0.5, 1, 0.4]
    - mask_area: A_r.reshape(1, -1) * (A_c).reshape(1, -1).T  = [[0.4, 0.2]
    [0.8, 0.4] [0.32, 0.16]]$\rightarrow$ mask_area.flatten $\rightarrow$ [0.4, 0.2, 0.8, 0.4, 0.32, 0.16]
    - mask index 순서에 따라 mask area 면적 할당하면 위의 이미지에 따른 결과가 나옴
- MOA module을 사용하는 것은 주어진 이미지에서 영역의 개수에 비례하여 연상량이 증가하기에 3가지 방식을 사용하여 추가적으로 연산량 줄임
    1. ViT의 마지막 layer에만 MOA module 추가
    2. CLS token이 extracted feature로 사용되기에 CLS token에 대해서만 attention score 계산
    3. query와 key 내적 한 번만 계산하고, 각각 복제된 mattention map에 $log(S)$ 더함<br>
    - $O(L^2\cdot C)$ $\rightarrow$ $O(M\cdot L \cdot C + M\cdot C^2)\qquad $ (L: patch 개수, C: hidden dimension, M: 영역 개수)<br>
    non-maximum suppression (NMS)에 의해 영역 개수가 제한되므로, patch의 개수에 대해서만 선형적인 관계를 갖게 됨

## 3.2 Pose-conditioned Graph Neural Network
### SCG Baseline
사람과 사물 nodes를 가진 bipartite graph neural network 
1. graph 초기화<br>
    - node encoding: ResNet과 ROIAlign을 사용
    - edge encoding: 사람과 사물의 bounding box의 공간적인 정보에 대한 특징값 사용
2. bidirectional message passing<br>
edge encoding에 따른 node들 사이에서의 bidirectional message passing
3. 사람과 사물 사이의 상호작용 판별<br>
update된 node encodings과 edge encodings을 사용하여 사람과 사물 사이의 상호작용 판별

### Pose-aware Edge Encoding
- 사람과 사물의 **공간적인 정보와 사람의 포즈**를 통해 상호작용성 식별 $\rightarrow$ graph 구조에서 human node와 object node 사이의 관계는 두 node 사이의 edge encoding으로 표현
- 초기화: edge encoding은 사람 정보와 공간적 정보를 사용하여 초기화
- "SCG처럼 pairwise spatial features(query)"와 pairwise joint features(key) 구함
    - pairwise joint features: hand-crafted feature (각 관절의 좌표 + 관절에서부터 object box 중점까지의 방향 벡터로 구성)를 MLP한 특징값 
- query와 key를 내적하여 사람의 각각의 관절에 대한 attention score 계산
    
    $$\alpha_{ij}=softmax(Q_{ij} K_{ij}^T \cdot s_i)$$

    - $\alpha_{ij}$: joint attention weight
    - $Q_{ij}$: pairwise spatial feature for the $i$ th human and $j$ th object
    - $K_{ij}$: pairwise joint feature for the $i$ th human and $j$ th object
    - $s_i$: pose estimation confidence score for the $i$ th human
- <span style='background-color:#fff5b1'>edge encoding은 처음에 pairwise spatial feature에서 시작하지만, 위의 attention 방법에 의해 pose-aware로 변해감</span>

### Message Passing with Pose
- 잡는 것(catch)과 붙잡는 것(grab) 경우를 구별하기 위해선 더 구체적인 local feature들이 필요함 $\rightarrow$ 사람의 HOI 인식 과정의 3번 째 단계와 연관됨 (사람의 특정한 관절에 대한 정보)
- human node encoding의 local information 사용 (human node encoding을 **사람의 각 관절에 대한 local 특징**을 사용하여  업데이트)
- 과정
    1. local feature 추출: 사람의 각 관절에 대한 local region box의 ViT output에 ROIAlign을 사용
    2. human local feature 계산: 사람 관절에 대한 local feature들의 weighted sum하여 human local feature 계산

        - $$x_{ij,local}=\sum_k \alpha_{ijk} \odot x_{ik,local}$$

            - $x_{ij,local}$: $i$ th human local feature for the $j$ th object
            - $\alpha_{ijk}$: $k$ th value of joint attention weight $\alpha_{ij}$
            - $x_{ik,local}$: local feature for the $k$ th joint of the $i$ th human
        - ViT output인 image patch embedding을 unflatten하여 ResNet output feature map처럼 ROIAlign 적용
            - CLS token보다 image patch token에 더 자세한 정보가 있다는 가정하에, MOA가 아닌 ROIAlign 사용
            - 학습 시에 CLS token은 classification head에 붙어있기에, image classification에 대한 정보가 coarse (거칠게)하게 존재
            - patch token들은 attention 방식을 통해 CLS token을 구성하므로 각 영역에 대해 fine(세세한) 정보 존재
            - **4.3에서 확인**
    3. human local feature $x_{ij,local}$로 human local node encoding 생성
        - human node encoding update 요소: human local feature + object node encoding + edge encoding
            
            $$M_{\mathcal{O} \rightarrow \mathcal{H}}(x_i^t,\ y^t_j,\ z_{ij})=MBF_{\mathcal{o}}(x_{ij,local} \oplus y^t_j,\ z_{ij})$$
            
            $$M_{\mathcal{H} \rightarrow \mathcal{O}} = MBF_{\mathcal{h}}(x_i^t,\ z_{ij})$$
            
            - $M_{\mathcal{O}\rightarrow \mathcal{H}}$: message function from object nodes to human nodes
            - $M_{\mathcal{H}\rightarrow \mathcal{O}}$: message function from human nodes to object nodes
            - $x_i^t$: $i$ th human node encoding at message passing step $t$
            - $y_j^t$: $j$ th object node encodingat message passing step $t$
            - MBF: multi-branch fusion module proposed in [49]
            - $\oplus$: concatenation
        - <span style='background-color:#fff5b1'>human node들은 human node와 비슷한 정보를 공유하는 human local features에 의해서도 update되는데, 이는 graph theory에서의 self-loop structure를 닮음</span>

### Effectiveness of Pose-Conditioned Graph
- **"사물 옆에 서 있기 vs 사물을 향해 손 뻗기" 상황이 주어졌을 때** 
- pose-aware edge encoding이 필요한 message passing에만 주목하도록 도움<br>
    사람과 사물이 같은 공간적 관계가 있지만, pose 정보가 서로 다르기에 edge-encoding이 다르고, 후자가 더 의미 있는 message pass
- human local feature들이 attention 연산을 통해 더 풍부한 local 정보를 포함할 수 있게 됨<br>
    후자의 경우에는 "손"
- 효과적인 HOI detection 가능<br> 
    필요한 local에 집중해야 함 $\rightarrow$ human node encoding은 self-loop 구조로 업데이트 $\rightarrow$ 풍부한 local 정보를 가진 human node encoding 생성됨 $\Rightarrow$ human node encoding이 사람에서 사물로 message를 전달하며 object node encoding 또한 향상시킴 (enrich)

## 3.3 Training and Inference
- SCG의 training & inference 과정 따름
    - training 시에 ground-truth boxes 덧붙임(appending)
    - detection 결과에 non-maximum suppression (NMS) 적용
    - focal loss를 구하기 위해 final HOI scores 계산

        $$s_k=(s_i^h)^{\lambda}\ \cdot\ (s_j^o)^{\lambda}\ \cdot\ \tilde{s}_k $$

        - $s_i^h$: $i$ th human detection score
        - $s_j^o$: $j$ th object detection score
        - $\tilde{s}_k$: action classification score (HOI triplet 표현에서 얻은 점수로 human node encoding, object node encoding, edge encoding으로 구성됨, 이런 encoding들은 MBF module에 의해 혼합됨)
        - $\lambda$: 1 (training), 2.8 (inference)
- focal loss 사용: 각 사람-사물 쌍의 가능한 상호작용을 학습하기 위한 multi-label classification loss

    $$FL(\hat{y}, y) = \begin{cases}
                            -\alpha(1-\hat{y})^{\gamma}\ log(\hat{y}),   & y = 1\\
                            -(1-\alpha)\hat{y}^{\gamma}\ log(1-\hat{y}), & y = 0
                        \end{cases}$$

    - $y$: ground-truth label
    - $\hat{y}$: final score for the human-object pair
    - $\alpha,\ \gamma$: balancing parameter (각각 0.5, 0.2로 설정)
- pose-conditioned graph에서 human과 object node에 해당하는 각 쌍의 edge encoding을 활용하여 interaction classification scores 구함
- CLS token $\rightarrow$ two-layer MLP $\rightarrow$ 각 node encoding 초기화
- 사람 포즈 + 공간 정보 $\rightarrow$ three-layer MLP $\rightarrow$ edge encoding
- 각 관절의 사람 local region box 크기는 bounding box 높이의 0.3배
- MBF module의 appearance feature을 위해 두 node encoding concatenate $\rightarrow$ human local node encodings와 object node encodings 사용하는 message function에서 사용
- AdamW
- learning rate: $10^{-4}$
- HICO-DET dataset: 8 epoch + flip data augmentation + learning rate decay (factor: 0.1)
- V-COCO dataset: 20 epoch + flip & color jittering data augmentation + 10 epoch마다 decay learning rate<br> pose information 사용 안 함
- batch size: 11 ($\text{ViPLO}_s$), 8 ($\text{ViPLO}_l$)
<br><br>

# 4. Experiments
## 4.1 Experimental Settings
### Datasets

| name  | # of samples | # of obj. categories | # of verb classes | HOI   | feat |
| :---: | :---:        | :---:                | :---:             | :---: | :---: |
| HICO-DET | 38,118(train)<br>9,658(test) | 80 | 117 | 600 types | more than 150k annotated human-object pairs|
| V-COCO | 2,533(train)<br>2,867(valid)<br>4,946(test) | 80 | 29 | - | not good for real world |

### Evaluation Metrics
- mean Average Precision (mAP): 사람과 사물의 bounding box IOU가 둘 다 ground truth box에 대해 0.5보다 큼 + HOI category 예측 정답
- HICO-DET
    - Full: all 600 HOI triplets, Rare + Non-Rare
    - Rare: training sample이 10개 미만인 138 HOI triplets
    - Non-Rare: training sample이 10개 이상인 462 HOI triplets
- V-COCO
    - Scenario 1: 가려진(occluded) 사물 bounding box에 대한 예측 포함
    - Scenario 2: 가려진(occluded) 사물 bounding box에 대한 예측 미포함

### Implementation Details
- pose에 대한 정답값이 없으므로, pose estimator는 Vitpose 사용 (MS-COCO Keypoint 데이터셋으로 학습됨) $\rightarrow$ human bounding box는 각각 17 keypoints 가짐
- backbone network 크기
    1. $\text{ViPLO}_s$: small version, ViT-B/32 (ViT base, patch 크기 32)
    2. $\text{ViPLO}_l$: large version, ViT-B/16 (ViT base, patch 크기 16)
- backbone 초기화: 사전 학습된 CLIP
    - [The devil is in the details: Delving into unbiased data processing for human pose estimation] 논문에 있는 data transformation으로 input 이미지를 $672\times 672$로 resize
    - bounding box와 human joint들도 이에 맞게 resize
    - ViT-B는 $224 \times 224$ input 크기로 사전 학습되어 있으나, [An image is worth 16x16 words: Transformers for image recognition at scale] 논문에 나온 방식처럼 **사전 학습된 position embedding의 2D interpolation**을 사용하여 $672\times 672$에 대해서도 fine-tune 가능

## 4.2 Comparison to State-of-the-Art
<p>
<img src="../assets/images/ViPLO/img_05.jpeg" width="100%">
</p> 

- HICO-DET 데이터셋에서 SOTA
- V-COCO 데이터셋에서 이전 SOTA와 비교할만한 결과를 보이지만, HICO-DET 데이터셋만큼의 차이를 보여주지 않음 $\rightarrow$ ViT는 적은 수의 데이터셋에 대해 치명적인 약점이 있기 때문
<br><br>

<p align='center'>
<img src="../assets/images/ViPLO/img_06.jpeg" width="60%">
</p> 

- two-stage 방식에서 ground truth bounding box가 제공되었을 때의 성능 비교
<br><br>

<p align='center'>
<img src="../assets/images/ViPLO/img_07.jpeg" width="60%">
</p> 

- ResNet-50을 backbone으로 사용하는 것과 속도와 메모리 효율성이 비슷하면서도 mAP는 더 잘 나옴
- ViPLO가 ResNet-50을 backbone으로 사용하는 SCG보다 GPU 메모리 사용량이 적음

## 4.3 Ablation Study
<p>
<img src="../assets/images/ViPLO/img_08.jpeg" width="100%">
</p> 

### MOA module: Table 4 (a)
- ViT + ROI: reshaped patch embedding을 사용하여 feature 추출 + bounding box에 대해 ROIAlign 사용
- $\text{ViT + MOA}^Q$: 겹친 면적 계산하지 않는 MOA module 사용하여, bounding box 경계가 지나가는 patch embedding mask out $\Rightarrow$ quantization 발생

---

- ViT만으로도 SCG baseline 성능 뛰어넘음
- MOA module의 quantization은 겹친 면적 계산하여 quantization이 없는 방법이 성능을 더 향상시킴

### Pose-conditioned Graph Neural Network: Table 4 (b)
- \+ pose edge: pose edge 추가하는 것만으로도 성능 향상
- \+ pose edge + local pose (MOA): human local features를 MOA module로 추출하면 성능을 하락시킴
- \+ pose edge + local pose (ROI): human local features를 ROIAlign 방법으로 추출하고, human local node encoding에 self-loop 구조를 넣으면 성능 향상

### CLIP pre-trained parameters
<p align='center'>
<img src="../assets/images/ViPLO/img_09.jpeg" width="60%">
</p> 

- CLIP이 COCO보다 데이터셋 크기가 더 큼
- CLIP ResNet50은 attention pooling 방법과 같은 요소들이 바뀌었기에 1대 1 비교 불가능
- 하지만, ViT + MOA 방식이 ResNet50보다 성능이 더 뛰어남을 보임

## 4.4 Additional Qualitative Results
<p align='center'>
<img src="../assets/images/ViPLO/img_10.jpeg" width="85%">
</p> 

- case 1: 사람과 사물의 거리가 멀어도 성공적으로 찾음
- case 2: 발목과 같은 사람의 특정 관절에 집중하여 상호작용 찾음
- case 3: 같은 사진에 대해서도 다른 사물에 대한 상호작용을 찾을 때 서로 다른 관절에 초점을 둠
<br><br>

# 개인적인 더 추가로 공부해야하는 것
1. confidence score에 대한 점
2. multi-branch fusion module(MBF)
3. 선행 연구들 (SCG, DEFR)
4. CLIP